<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://dsolnik.github.io/blog/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dsolnik.github.io/blog/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-29T00:15:53+00:00</updated><id>https://dsolnik.github.io/blog/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Upper Confidence Bounds for Multi-Arm Bandits</title><link href="https://dsolnik.github.io/blog/blog/2023/UCB-derivation/" rel="alternate" type="text/html" title="Upper Confidence Bounds for Multi-Arm Bandits"/><published>2023-12-28T15:12:00+00:00</published><updated>2023-12-28T15:12:00+00:00</updated><id>https://dsolnik.github.io/blog/blog/2023/UCB-derivation</id><content type="html" xml:base="https://dsolnik.github.io/blog/blog/2023/UCB-derivation/"><![CDATA[<p><a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton and Barto</a> do a great job of explaining many things but the derivation of Upper Confidence Bounds is not great.</p> <p>Let’s get to whether they got differently.</p> <h3 id="problems-with-epsilon-greedy-algorithms">Problems with \(\epsilon\)-greedy algorithms</h3> <p>An \(\epsilon\)-greedy solution explores \(\epsilon\)% of the time and selects the <em>greedy</em> solution the other \(1 - \epsilon\)% of the time.</p> <p>If we assume that the means for the different “arms” are stationary, i.e. not changing over time, \(\epsilon\)-greedy algorithms will <em>always</em>, even after we’ve been training for eons, select every action \(\frac{\epsilon}{k}\)% of the time.</p> <p>Ideally, we want to explore a lot at the beginning and exploit more as we sample more from the different arms. An $\epsilon$-greedy algorithm makes no distinction between when we have more information and when we have less information.</p> <h3 id="vanilla-upper-confidence-bounds">Vanilla Upper Confidence Bounds</h3> <p>When we get more samples, we learn more about the variance of the different arms.</p> <p>One way to formalize this is to construct a confidence interval and then pick the arm which has the highest upper confidence bound.</p> <p>Lets say that we’re using a \(95\)% confidence interval. Lets use the Central Limit Theorem to get a confidence interval for the rewards from an action \(a\):</p> \[\overline{x} \pm \Phi(.025)\cdot \frac{\sigma_{R_a}}{\sqrt{n}}\] <p>This suggests our vanilla Upper Confidence Bound Algorithm, which will be to pick the action with the highest \(95\)% upper confidence bound. \(N_t(a)\) here will be the number of times we selected action \(a\) before time \(t\). \(Q_t(a)\) will be the estimate we have for the mean of arm \(a\) at time \(t\).</p> \[A_{t}\doteq \underset{a}{\arg\max}\left[Q_{t}(a) + \Phi(.975)\cdot \sigma_{R_a} \frac{1}{\sqrt{N_t(a)}}\right]\] <p>OK, cool, but there’s one glaring problem here: We don’t know \(\sigma_{R_a}\). We could estimate it using the sample variance and use a quantile in the \(t\)-distribution, but what if we just <em>gasps</em> make it a <strong>hyper-parameter</strong> and make setting it our user’s problems? Lets call that \(c_a\)</p> <p>And while we’re at it, collapse \(\Phi(.975)\) into the constant too.</p> \[\begin{align} A_{t} &amp;\doteq \underset{a}{\arg\max}\left[Q_{t}(a) + \Phi(.975)\cdot \sigma_{R_a} \frac{1}{\sqrt{N_t(a)}}\right] \\ &amp;=\underset{a}{\arg\max}\left[Q_{t}(a) + c_a \frac{1}{\sqrt{N_t(a)}}\right] \end{align}\] <h3 id="agumenting-upper-confidence-bounds-for-exploration">Agumenting Upper Confidence Bounds for Exploration</h3> <p>Now, a problem with this is that you might get extraordinarily unlucky and pull a couple really low rewards for the optimal arm.</p> <p>To deal with that, one elegant way is to artificially increase the standard deviations we use in the confidence bounds as we increase time. We can do this with a multipler \(\lambda_a(t)\).</p> \[A_{t} =\underset{a}{\arg\max}\left[Q_{t}(a) + c_a \cdot \lambda_a(t) \cdot \frac{1}{\sqrt{N_t(a)}}\right]\] <p>Well, we want \(\lambda_a(t)\) to decrease over time, right? But, we want to make sure that it’s always unbounded so that it gives any estimate the ability to recover from a really unlucky streak. What if we set it to \(\lambda_a(t) = \sqrt{\ln t}\).</p> <p>Then, we get</p> \[\begin{align} A_{t} &amp;=\underset{a}{\arg\max}\left[Q_{t}(a) + c_a \cdot \lambda_a(t) \cdot \frac{1}{\sqrt{N_t(a)}}\right] \\ &amp;= \underset{a}{\arg\max}\left[Q_{t}(a) + c_a \cdot \sqrt{\ln t} \cdot \frac{1}{\sqrt{N_t(a)}}\right] \\ &amp;= \underset{a}{\arg\max}\left[Q_{t}(a) + c_a \cdot \sqrt{\frac{\ln t}{N_t(a)}}\right] \end{align}\] <p>This is the final formulation in the book by Sutton and Barto.</p>]]></content><author><name></name></author><category term="math"/><category term="statistics"/><summary type="html"><![CDATA[Deriving Sutton and Barton's UCB Bandit Algoritmhs]]></summary></entry></feed>